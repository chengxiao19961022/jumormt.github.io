{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Copyright © 程潇 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 15:59:34 "},"Chapter1/Basic.html":{"url":"Chapter1/Basic.html","title":"基本概念","keywords":"","body":"基本概念 多任务 在上古时代，CPU 资源十分昂贵，如果让 CPU 只能运行一个程序，那么当 CPU 空闲下来（例如等待 I/O 时），CPU 就空闲下来了。为了让 CPU 得到更好的利用，人们编写了一个监控程序，如果发现某个程序暂时无须使用 CPU 时，监控程序就把另外的正在等待 CPU 资源的程序启动起来，以充分利用 CPU 资源。这种方法被称为 多道程序（Multiprogramming）。 对于多道程序来说，最大的问题是程序之间不区分轻重缓急，对于交互式程序来说，对于 CPU 计算时间的需求并不多，但是对于响应速度却有比较高的要求。而对于计算类程序来说则正好相反，对于响应速度要求低，但是需要长时间的 CPU 计算。想象一下我们同时在用浏览器上网和听音乐，我们希望浏览器能够快速响应，同时也希望音乐不停掉。这时候多道程序就没法达到我们的要求了。于是人们改进了多道程序，使得每个程序运行一段时间之后，都主动让出 CPU 资源，这样每个程序在一段时间内都有机会运行一小段时间。这样像浏览器这样的交互式程序就能够快速地被处理，同时计算类程序也不会受到很大影响。这种程序协作方式被称为 分时系统（Time-Sharing System）。 在分时系统的帮助下，我们可以边用浏览器边听歌了，但是如果某个程序出现了错误，导致了死循环，不仅仅是这个程序会出错，整个系统都会死机。为了避免这种情况，一个更为先进的操作系统模式被发明出来，也就是我们现在很熟悉的 多任务（Multi-tasking）系统。操作系统从最底层接管了所有硬件资源。所有的应用程序在操作系统之上以 进程（Process） 的方式运行，每个进程都有自己独立的地址空间，相互隔离。CPU 由操作系统统一进行分配。每个进程都有机会得到 CPU，同时在操作系统控制之下，如果一个进程运行超过了一定时间，就会被暂停掉，失去 CPU 资源。这样就避免了一个程序的错误导致整个系统死机。如果操作系统分配给各个进程的运行时间都很短，CPU 可以在多个进程间快速切换，就像很多进程都同时在运行的样子。几乎所有现代操作系统都是采用这样的方式支持多任务，例如 Unix，Linux，Windows 以及 macOS。 进程 进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。 进程的概念主要有两点：第一，进程是一个实体。每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。第二，进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时，它才能成为一个活动的实体，我们称其为进程。 进程的基本状态 等待态：等待某个事件的完成； 就绪态：等待系统分配处理器以便运行； 运行态：占有处理器正在运行。 运行态→等待态 往往是由于等待外设，等待主存等资源分配或等待人工干预而引起的。 等待态→就绪态 则是等待的条件已满足，只需分配到处理器后就能运行。 运行态→就绪态 不是由于自身原因，而是由外界原因使运行状态的进程让出处理器，这时候就变成就绪态。例如时间片用完，或有更高优先级的进程来抢占处理器等。 就绪态→运行态 系统按某种策略选中就绪队列中的一个进程占用处理器，此时就变成了运行态 进程调度 调度种类 高级、中级和低级调度作业从提交开始直到完成，往往要经历下述三级调度： 高级调度：(High-Level Scheduling)又称为作业调度，它决定把后备作业调入内存运行； 中级调度：(Intermediate-Level Scheduling)又称为在虚拟存储器中引入，在内、外存对换区进行进程对换。 低级调度：(Low-Level Scheduling)又称为进程调度，它决定把就绪队列的某进程获得CPU； 非抢占式调度与抢占式调度 非抢占式 分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或发生进程调度进程调度某事件而阻塞时，才把处理机分配给另一个进程。 抢占式 操作系统将正在运行的进程强行暂停，由调度程序将CPU分配给其他就绪进程的调度方式。 调度策略的设计 响应时间: 从用户输入到产生反应的时间 周转时间: 从任务开始到任务结束的时间 CPU任务可以分为交互式任务和批处理任务，调度最终的目标是合理的使用CPU，使得交互式任务的响应时间尽可能短，用户不至于感到延迟，同时使得批处理任务的周转时间尽可能短，减少用户等待的时间。 调度算法 FIFO或First Come, First Served (FCFS) 调度的顺序就是任务到达就绪队列的顺序。 公平、简单(FIFO队列)、非抢占、不适合交互式。未考虑任务特性，平均等待时间可以缩短 Shortest Job First (SJF) 最短的作业(CPU区间长度最小)最先调度。 可以证明，SJF可以保证最小的平均等待时间。 Shortest Remaining Job First (SRJF) SJF的可抢占版本，比SJF更有优势。 SJF(SRJF): 如何知道下一CPU区间大小？根据历史进行预测: 指数平均法。 优先权调度 每个任务关联一个优先权，调度优先权最高的任务。 注意：优先权太低的任务一直就绪，得不到运行，出现“饥饿”现象。 FCFS是RR的特例，SJF是优先权调度的特例。这些调度算法都不适合于交互式系统。 Round-Robin(RR) 设置一个时间片，按时间片来轮转调度（“轮叫”算法） 优点: 定时有响应，等待时间较短；缺点: 上下文切换次数较多； 如何确定时间片？ 时间片太大，响应时间太长；吞吐量变小，周转时间变长；当时间片过长时，退化为FCFS。 多级队列调度 按照一定的规则建立多个进程队列 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法 存在问题1：没法区分I/O bound和CPU bound； 存在问题2：也存在一定程度的“饥饿”现象； 多级反馈队列 在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务。 可以根据“享用”CPU时间多少来移动队列，阻止“饥饿”。 最通用的调度算法，多数OS都使用该方法或其变形，如UNIX、Windows等。 进程同步 临界资源与临界区 在操作系统中，进程是占有资源的最小单位（线程可以访问其所在进程内的所有资源，但线程本身并不占有资源或仅仅占有一点必须资源）。但对于某些资源来说，其在同一时间只能被一个进程所占用。这些一次只能被一个进程所占用的资源就是所谓的临界资源。典型的临界资源比如物理上的打印机，或是存在硬盘或内存中被多个进程所共享的一些变量和数据等(如果这类资源不被看成临界资源加以保护，那么很有可能造成丢数据的问题)。 对于临界资源的访问，必须是互斥进行。也就是当临界资源被占用时，另一个申请临界资源的进程会被阻塞，直到其所申请的临界资源被释放。而进程内访问临界资源的代码被成为临界区。 对于临界区的访问过程分为四个部分： 进入区:查看临界区是否可访问，如果可以访问，则转到步骤二，否则进程会被阻塞 临界区:在临界区做操作 退出区:清除临界区被占用的标志 剩余区：进程与临界区不相关部分的代码 解决临界区问题可能的方法： 一般软件方法 关中断方法 硬件原子指令方法 信号量方法 信号量 信号量是一个确定的二元组（s，q），其中s是一个具有非负初值的整形变量，q是一个初始状态为空的队列，整形变量s表示系统中某类资源的数目： 当其值 ≥ 0 时，表示系统中当前可用资源的数目 当其值 ＜ 0 时，其绝对值表示系统中因请求该类资源而被阻塞的进程数目 除信号量的初值外，信号量的值仅能由P操作和V操作更改，操作系统利用它的状态对进程和资源进行管理 P操作： P操作记为P(s)，其中s为一信号量，它执行时主要完成以下动作： s.value = s.value - 1； /*可理解为占用1个资源，若原来就没有则记帐“欠”1个*/ 若s.value ≥ 0，则进程继续执行，否则（即s.value 说明：实际上，P操作可以理解为分配资源的计数器，或是使进程处于等待状态的控制指令 V操作： V操作记为V(s)，其中s为一信号量，它执行时，主要完成以下动作： s.value = s.value + 1；/*可理解为归还1个资源，若原来就没有则意义是用此资源还1个欠帐*/ 若s.value > 0，则进程继续执行，否则（即s.value ≤ 0）,则从信号量s的等待队s.queue中移出第一个进程，使其变为就绪状态，然后返回原进程继续执行 说明：实际上，V操作可以理解为归还资源的计数器，或是唤醒进程使其处于就绪状态的控制指令 信号量方法实现：生产者 − 消费者互斥与同步控制 semaphore fullBuffers = 0; /*仓库中已填满的货架个数*/ semaphore emptyBuffers = BUFFER_SIZE;/*仓库货架空闲个数*/ semaphore mutex = 1; /*生产-消费互斥信号*/ Producer() { while(True) { /*生产产品item*/ emptyBuffers.P(); mutex.P(); /*item存入仓库buffer*/ mutex.V(); fullBuffers.V(); } } Consumer() { while(True) { fullBuffers.P(); mutex.P(); /*从仓库buffer中取产品item*/ mutex.V(); emptyBuffers.V(); /*消费产品item*/ } } 死锁 死锁: 多个进程因循环等待资源而造成无法执行的现象。 死锁会造成进程无法执行，同时会造成系统资源的极大浪费(资源无法释放)。 死锁产生的4个必要条件： 互斥使用(Mutual exclusion) 指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 不可抢占(No preemption) 指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 请求和保持(Hold and wait) 指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 循环等待(Circular wait) 指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 死锁避免——银行家算法 思想: 判断此次请求是否造成死锁若会造成死锁，则拒绝该请求 进程间通信 本地进程间通信的方式有很多，可以总结为下面四类： 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC） 线程 多进程解决了前面提到的多任务问题。然而很多时候不同的程序需要共享同样的资源（文件，信号量等），如果全都使用进程的话会导致切换的成本很高，造成 CPU 资源的浪费。于是就出现了线程的概念。 线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。 线程具有以下属性： 轻型实体 线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。 独立调度和分派的基本单位。 在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 可并发执行。 在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 共享进程资源。 在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。 使用 pthread 线程库实现的生产者－消费者模型： #include #include #include #define BUFFER_SIZE 10 static int buffer[BUFFER_SIZE] = { 0 }; static int count = 0; pthread_t consumer, producer; pthread_cond_t cond_producer, cond_consumer; pthread_mutex_t mutex; void* consume(void* _){ while(1){ pthread_mutex_lock(&mutex); while(count == 0){ printf(\"empty buffer, wait producer\\n\"); pthread_cond_wait(&cond_consumer, &mutex); } count--; printf(\"consume a item\\n\"); pthread_mutex_unlock(&mutex); pthread_cond_signal(&cond_producer); //pthread_mutex_unlock(&mutex); } pthread_exit(0); } void* produce(void* _){ while(1){ pthread_mutex_lock(&mutex); while(count == BUFFER_SIZE){ printf(\"full buffer, wait consumer\\n\"); pthread_cond_wait(&cond_producer, &mutex); } count++; printf(\"produce a item.\\n\"); pthread_mutex_unlock(&mutex); pthread_cond_signal(&cond_consumer); //pthread_mutex_unlock(&mutex); } pthread_exit(0); } int main() { pthread_mutex_init(&mutex, NULL); pthread_cond_init(&cond_consumer, NULL); pthread_cond_init(&cond_producer, NULL); int err = pthread_create(&consumer, NULL, consume, (void*)NULL); if(err != 0){ printf(\"consumer thread created failed\\n\"); exit(1); } err = pthread_create(&producer, NULL, produce, (void*)NULL); if(err != 0){ printf(\"producer thread created failed\\n\"); exit(1); } pthread_join(producer, NULL); pthread_join(consumer, NULL); //sleep(1000); pthread_cond_destroy(&cond_consumer); pthread_cond_destroy(&cond_producer); pthread_mutex_destroy(&mutex); return 0; } 锁 这里讨论的主要是多线程编程中需要使用的锁，网上有关于锁的文章实在是非常多而且乱套，让新手不知道从何下手。这里我们不去钻名词和概念的牛角尖，而是直接从本质上试图解释一下锁这个很常用的多线程编程工具。 锁要解决的是线程之间争取资源的问题，这个问题大概有下面几个角度： 资源是否是独占（独占锁 - 共享锁） 抢占不到资源怎么办（互斥锁 - 自旋锁） 自己能不能重复抢（重入锁 - 不可重入锁） 竞争读的情况比较多，读可不可以不加锁（读写锁） 上面这几个角度不是互相独立的，在实际场景中往往要它们结合起来，才能构造出一个合适的锁。 独占锁 - 共享锁 当一个共享资源只有一份的时候，通常我们使用独占锁，常见的即各个语言当中的 Mutex。当共享资源有多份时，可以使用前面提到的 Semaphere。 互斥锁 - 自旋锁 对于互斥锁来说，如果一个线程已经锁定了一个互斥锁，第二个线程又试图去获得这个互斥锁，则第二个线程将被挂起（即休眠，不占用 CPU 资源）。 在计算机系统中，频繁的挂起和切换线程，也是有成本的。自旋锁就是解决这个问题的。 自旋锁，指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 容易看出，当资源等待的时间较长，用互斥锁让线程休眠，会消耗更少的资源。当资源等待的时间较短时，使用自旋锁将减少线程的切换，获得更高的性能。 较新版本的 Java 中的 synchornized 和 .NET 中的 lock（Monitor） 的实现，是结合了两种锁的特点。简单说，它们在发现资源被抢占之后，会先试着自旋等待一段时间，如果等待时间太长，则会进入挂起状态。通过这样的实现，可以较大程度上挖掘出锁的性能。 重入锁 - 不可重入锁 可重入锁（ReetrantLock），也叫做递归锁，指的是在同一线程内，外层函数获得锁之后，内层递归函数仍然可以获取到该锁。换一种说法：同一个线程再次进入同步代码时，可以使用自己已获取到的锁。 使用可重入锁时，在同一线程中多次获取锁，不会导致死锁。使用不可重入锁，则会导致死锁发生。 Java 中的 synchornized 和 .NET 中的 lock（Monitor） 都是可重入的。 读写锁 有些情况下，对于共享资源读竞争的情况远远多于写竞争，这种情况下，对读操作每次都进行加速，是得不偿失的。读写锁就是为了解决这个问题。 读写锁允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。简单可以总结为，读读不互斥，读写互斥，写写互斥。 对读写锁来说，有一个升级和降级的概念，即当前获得了读锁，想把当前的锁变成写锁，称为升级，反之称为降级。锁的升降级本身也是为了提升性能，通过改变当前锁的性质，避免重复获取锁。 协程 协程，又称微线程，纤程。英文名 Coroutine。 协程可以理解为用户级线程，协程和线程的区别是：线程是抢占式的调度，而协程是协同式的调度，协程避免了无意义的调度，由此可以提高性能，但也因此，程序员必须自己承担调度的责任，同时，协程也失去了标准线程使用多CPU的能力。 使用协程(python)改写生产者-消费者问题： import time def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) time.sleep(1) r = '200 OK' def produce(c): next(c) #python 3.x中使用next(c)，python 2.x中使用c.next() n = 0 while n 可以看到，使用协程不再需要显式地对锁进行操作。 IO多路复用 基本概念 IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合： 当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用。 当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。 如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。 如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。 如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 常见的IO复用实现 select(Linux/Windows/BSD) epoll(Linux) kqueue(BSD/Mac OS X) 参考资料 浅谈进程同步与互斥的概念 进程间同步——信号量 百度百科：线程 进程、线程和协程的理解 协程 IO多路复用之select总结 银行家算法 Copyright © 程潇 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-24 21:08:13 "},"Chapter1/Summary.html":{"url":"Chapter1/Summary.html","title":"知识点总结","keywords":"","body":"知识点总结 并发编程的优缺点 优点 并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升； 面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分。 缺点 频繁的上下文切换 (特别的，cpu由于核心有限，往往是通过时分复用的方式实现并行多线程，会涉及大量的内存拷贝和上下文切换开销) 解决思路 无锁并发编程：可以参照concurrentHashMap锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。 CAS算法，利用Atomic下使用CAS算法来更新数据，使用了乐观锁，可以有效的减少一部分不必要的锁竞争带来的上下文切换 使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 可以使用Lmbench3测量上下文切换的时长 vmstat测量上下文切换次数 线程安全问题 产生本质在于： 主内存和java线程工作内存的不一致性；（为什么需要cpu缓存？因为cpu的频率太快了，主存跟不上，cache是为了缓解两者速度不匹配这一问题） 指令重排序。（处理器为提高运算速度而做出违背代码原有顺序的优化） 上述内容后面会仔细展开 如死锁问题 public class DeadLockDemo { private static String resource_a = \"A\"; private static String resource_b = \"B\"; public static void main(String[] args) { deadLock(); } public static void deadLock() { Thread threadA = new Thread(new Runnable() { @Override public void run() { synchronized (resource_a) { System.out.println(\"get resource a\"); try { Thread.sleep(3000); synchronized (resource_b) { System.out.println(\"get resource b\"); } } catch (InterruptedException e) { e.printStackTrace(); } } } }); Thread threadB = new Thread(new Runnable() { @Override public void run() { synchronized (resource_b) { System.out.println(\"get resource b\"); synchronized (resource_a) { System.out.println(\"get resource a\"); } } } }); threadA.start(); threadB.start(); } } 避免死锁的方法： 避免一个线程同时获得多个锁； 避免一个线程在锁内部占有多个资源，尽量保证每个锁只占用一个资源； 尝试使用定时锁，使用lock.tryLock(timeOut)，当超时等待时当前线程不会阻塞； 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况 一些基本概念对比 同步VS异步 同步方法调用一开始，调用者必须等待被调用的方法结束后，调用者后面的代码才能执行。而异步调用，指的是，调用者不用管被调用方法是否完成，都会继续执行后面的代码。 并发与并行 并发指的是多个任务交替进行，而并行则是指真正意义上的“同时进行”。 临界区 临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用。但是每个线程使用时，一旦临界区资源被一个线程占有，那么其他线程必须等待。 阻塞和非阻塞 阻塞和非阻塞通常用来形容多线程间的相互影响，比如一个线程占有了临界区资源，那么其他线程需要这个资源就必须进行等待该资源的释放，会导致等待的线程挂起，这种情况就是阻塞，而非阻塞就恰好相反，它强调没有一个线程可以阻塞其他线程，所有的线程都会尝试地往前运行。 线程的状态转换以及基本操作 新建线程 实际上java程序天生就是一个多线程程序，包含了：（1）分发处理发送给给JVM信号的线程；（2）调用对象的finalize方法的线程；（3）清除Reference的线程；（4）main线程 实现多线程的方式： 继承Thread类，使用run方法进行同步启动 实现Runnable接口，使用start方法进行异步启动 使用ExecutorService的exec(runnable)方法运行runnable类 使用ExecutorService的submit(runnable/callable)，启动返回结果的线程，返回值为Future，再调用get()来获得结果。 Thread和Runable的区别和联系（看看就好）： 联系： Thread类实现了Runable接口。 都需要重写里面Run方法。 不同： 实现Runnable的类更具有健壮性，避免了单继承的局限。 Runnable更容易实现资源共享，能多个线程同时处理一个资源。 直接继承Thread类会继承所有方法，开销较大 问题:为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法？ 用start()来启动线程 ---> 异步执行 而如果使用run()来启动线程 ---> 同步执行 多线程就是为了并发执行，因此需要使用start 线程状态 wait(),join(),LockSupport.lock()方法线程会进入到WAITING(阻塞等待态) wait(long timeout)，sleep(long),join(long),LockSupport.parkNanos(),LockSupport.parkUtil()方法线程会进入到TIMED_WAITING,当超时等待时间到达后，线程会切换到Runable的状态 WAITING和TIMED _WAITING状态时可以通过Object.notify(),Object.notifyAll()方法使线程转换到Runable状态 当线程出现资源竞争时，即等待获取锁的时候，线程会进入到BLOCKED阻塞状态(位于AQS同步队列),当线程获取锁时，线程进入到Runable状态 线程运行结束后，线程进入到TERMINATED状态 当线程进入到synchronized方法或者synchronized代码块时，线程切换到的是BLOCKED状态，而使用java.util.concurrent.locks下lock进行加锁的时候线程切换的是WAITING或者TIMED_WAITING状态，因为lock会调用LockSupport的方法。 线程状态的基本操作 interrupted 其他线程可以调用该线程的interrupt()方法对其进行中断操作，同时该线程可以调用 isInterrupted()来感知其他线程对其自身的中断操作，从而做出响应。另外，同样可以调用Thread的静态方法 interrupted()对当前线程进行中断操作，该方法会清除中断标志位。需要注意的是，当抛出InterruptedException时候，会清除中断标志位，也就是说在调用isInterrupted会返回false。 一般在结束线程时通过中断标志位或者标志位的方式可以有机会去清理资源，相对于武断而直接的结束线程，这种方式要优雅和安全。 if(this.isInterrupted()){ close();//清理资源 } join 如果一个线程实例A执行了threadB.join(),其含义是：当前线程A会等待threadB线程终止后threadA才会继续执行。另外还提供了超时等待。 当threadB退出时会调用notifyAll()方法通知所有的等待线程(join让进程进入waiting状态)。 sleep sleep() VS wait(): sleep()方法是Thread的静态方法，而wait()是Object实例方法。 wait()方法必须要在同步方法或者同步块中调用（wait/notify方法依赖于moniterenter和moniterexit）也就是必须已经获得对象锁。而sleep()方法没有这个限制可以在任何地方种使用。另外，wait()方法会释放占有的对象锁，使得该线程进入等待池中，等待下一次获取资源。而sleep()方法只是会让出CPU并不会释放掉对象锁； sleep()方法在休眠时间达到后如果再次获得CPU时间片就会继续执行，而wait()方法必须等待Object.notift/Object.notifyAll通知后，才会离开等待池，并且再次获得CPU时间片才会继续执行。 yield 这是一个静态方法，一旦执行，它会是当前线程让出CPU。 让出的CPU并不是代表当前线程不再运行了，如果在下一次竞争中，又获得了CPU时间片当前线程依然会继续运行,让出的时间片只会分配给当前线程相同优先级的线程。 在Java程序中，通过一个整型成员变量Priority来控制优先级，优先级的范围从1~10.在构建线程的时候可以通过setPriority(int)方法进行设置，默认优先级为5，优先级高的线程相较于优先级低的线程优先获得处理器时间片。 守护线程Daemon 守护线程是一种特殊的线程，就和它的名字一样，它是系统的守护者，在后台默默地守护一些系统服务，比如垃圾回收线程，JIT线程就可以理解守护线程。与之对应的就是用户线程，用户线程就可以认为是系统的工作线程，它会完成整个系统的业务操作。用户线程完全结束后就意味着整个系统的业务任务全部结束了，因此系统就没有对象需要守护的了，守护线程自然而然就会退。当一个Java应用，只有守护线程的时候，虚拟机就会自然退出。 public class DaemonDemo { public static void main(String[] args) { Thread daemonThread = new Thread(new Runnable() { @Override public void run() { while (true) { try { System.out.println(\"i am alive\"); Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(\"finally block\"); } } } }); daemonThread.setDaemon(true); daemonThread.start(); //确保main线程结束前能给daemonThread能够分到时间片 try { Thread.sleep(800); } catch (InterruptedException e) { e.printStackTrace(); } } } 这里需要注意的是守护线程在退出的时候并不会执行finnaly块中的代码，所以将释放资源等操作不要放在finnaly块中执行，这种操作是不安全的 Java内存模型以及happens-before规则 前面说到，出现线程安全的问题一般是因为主内存和工作内存数据不一致性和重排序导致的。 内存模型抽象结构 在并发编程中主要需要解决两个问题：1. 线程之间如何通信；2.线程之间如何完成同步（这里的线程指的是并发执行的活动实体）。通信是指线程之间以何种机制来交换信息，主要有两种：共享内存和消息传递。 java内存模型是共享内存的并发模型，线程之间主要通过读-写共享变量来完成隐式通信。 哪些是共享变量 在java程序中所有实例域，静态域和数组元素都是放在堆内存中（所有线程均可访问到，是可以共享的），而局部变量，方法定义参数和异常处理器参数不会在线程间共享。共享数据会出现线程安全的问题，而非共享数据不会出现线程安全的问题。关于JVM运行时内存区域在后面的文章会讲到。 JMM抽象结构模型 CPU的处理速度和主存的读写速度不是一个量级的，为了平衡这种巨大的差距，每个CPU都会有缓存。共享变量会先放在主存中，每个线程都有属于自己的工作内存，并且会把位于主存中的共享变量拷贝到自己的工作内存，之后的读写操作均使用位于工作内存的变量副本 线程A和线程B之间要完成通信的话，要经历如下两步： 线程A从主内存中将共享变量读入线程A的工作内存后并进行操作，之后将数据重新写回到主内存中； 线程B从主存中读取最新的共享变量 重排序 为了提高性能，编译器和处理器常常会对指令进行重排序。 一般重排序可以分为如下三种： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序； 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序； 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行的。 double pi = 3.14 //A double r = 1.0 //B double area = pi * r * r //C 这是一个计算圆面积的代码，由于A,B之间没有任何关系，对最终结果也不会存在关系，它们之间执行顺序可以重排序。因此可以执行顺序可以是A->B->C或者B->A->C执行最终结果都是3.14，即A和B之间没有数据依赖性。 如果两个操作访问同一个变量，且这两个操作有一个为写操作，此时这两个操作就存在数据依赖性这里就存在三种情况：1. 读后写；2.写后写；3. 写后读，这三种操作都是存在数据依赖性的，如果重排序会对最终执行结果会存在影响。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序。 as-if-serial 不管怎么重排序（编译器和处理器为了提供并行度），（单线程）程序的执行结果不能被改变。编译器，runtime和处理器都必须遵守as-if-serial语义。as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器，runtime和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。 happens-before规则 JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证。 as-if-serial VS happens-before： 不同点： as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。 相同点：as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 具体的一共有8项规则： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。(代码顺序) 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。（syncronized） volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。（先start再有线程的操作） join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 程序中断规则：对线程interrupted()方法的调用先行于被中断线程的代码检测到中断时间的发生。 对象finalize规则：一个对象的初始化完成（构造函数执行结束）先行于发生它的finalize()方法的开始。 JMM的设计 在设计JMM时需要考虑两个关键因素: 程序员对内存模型的使用 程序员希望内存模型易于理解、易于编程。程序员希望基于一个强内存模型来编写代码。 编译器和处理器对内存模型的实现 编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。 实际上： JMM向程序员提供的happens-before规则能满足程序员的需求。JMM的happens-before规则不但简单易懂，而且也向程序员提供了足够强的内存可见性保证（有些内存可见性保证其实并不一定真实存在，比如上面的A happens-before B）。 JMM对编译器和处理器的束缚已经尽可能少。从上面的分析可以看出，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。例如，如果编译器经过细致的分析后，认定一个锁只会被单个线程访问，那么这个锁可以被消除。再如，如果编译器经过细致的分析后，认定一个volatile变量只会被单个线程访问，那么编译器可以把这个volatile变量当作一个普通变量来对待。这些优化既不会改变程序的执行结果，又能提高程序的执行效率。 彻底理解synchronized 实现原理 synchronized可以用在方法上也可以使用在代码块中，其中方法是实例方法和静态方法分别锁的是该类的实例对象和该类的对象。而使用在代码块中也可以分为三种，具体的可以看上面的表格。这里的需要注意的是：如果锁的是类对象的话，尽管new多个实例对象，但他们仍然是属于同一个类依然会被锁住，即线程之间保证同步关系。 执行同步代码块后首先要先执行monitorenter指令，退出的时候monitorexit指令.当线程获取monitor后才能继续往下执行，否则就只能等待。而这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到monitor。 monitorenter ： 每个对象都有一个monitor锁，包含线程持有者和计数器。 1.如果计数器为0，则该线程进入monitor，然后将计数器设置为1，该线程即为monitor的所有者。 2.如果线程已经占有该monitor，重新进入，则计数器加1. 3.如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到计数器为0。 monitorexit： 1. 指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor。 2. 其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权。 通过这两段描述，我们应该能很清楚的看出Synchronized的实现原理，Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。 锁的重入性，即在同一锁程中，线程不需要再次获取同一把锁。Synchronized先天具有重入性。每个对象拥有一个计数器，当线程获取该对象锁后，计数器就会加一，释放锁后就会将计数器减一。(再次进入计数器加1，计数器为0时释放) synchronized的happens-before关系 对同一个监视器的解锁，happens-before于对该监视器的加锁 在图中每一个箭头连接的两个节点就代表之间的happens-before关系，黑色的是通过程序顺序规则推导出来，红色的为监视器锁规则推导而出：线程A释放锁happens-before线程B加锁，蓝色的则是通过程序顺序规则和监视器锁规则推测出来happens-befor关系，通过传递性规则进一步推导的happens-before关系。现在我们来重点关注2 happens-before 5，通过这个关系我们可以得出什么？ 根据happens-before的定义中的一条:如果A happens-before B，则A的执行结果对B可见，并且A的执行顺序先于B。线程A先对共享变量A进行加一，由2 happens-before 5关系可知线程A的执行结果对线程B可见即线程B所读取到的a的值为1。 锁获取和锁释放的内存语义 从整体上来看，线程A的执行结果（a=1）对线程B是可见的，实现原理为：释放锁的时候会将值刷新到主内存中，其他线程获取锁时会强制从主内存中获取最新的值。另外也验证了2 happens-before 5，2的执行结果对5是可见的。 从横向来看，这就像线程A通过主内存中的共享变量和线程B进行通信，A 告诉 B 我们俩的共享数据现在为1啦，这种线程间的通信机制正好吻合java的内存模型正好是共享内存的并发模型结构。 synchronized优化 CAS操作 CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程。 在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现. Synchronized VS CAS 元老级的Synchronized(未优化前)最主要的问题是：在存在线程竞争的情况下会出现线程阻塞和唤醒锁带来的性能问题(进入了内核态)，因为这是一种互斥同步（阻塞同步）。而CAS并不是武断的间线程挂起，当CAS操作失败后会进行一定的尝试，而非进行耗时的挂起唤醒的操作，因此也叫做非阻塞同步。这是两者主要的区别。 CAS的问题 ABA问题 因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。原来的变化路径A->B->A就变成了1A->2B->3C。java这么优秀的语言，当然在java 1.5后的atomic包中提供了AtomicStampedReference来解决ABA问题，解决思路就是这样的。 自旋时间过长 使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋（无非就是一个死循环）进行下一次尝试，如果这里自旋时间过长对性能是很大的消耗。如果JVM能支持处理器提供的pause指令，那么在效率上会有一定的提升。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时CAS能保证其原子性，如果对多个共享变量进行操作,CAS就不能保证其原子性。有一个解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。 彻底理解volatile 被volatile修饰的变量能够保证每个线程能够获取该变量的最新值，从而避免出现数据脏读的现象。 实现原理 在生成汇编代码时会在volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令。 主要有这两个方面的影响： 将当前处理器缓存行的数据写回系统内存； 这个写回内存的操作会使得其他CPU里缓存了该内存地址的数据无效 在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 Lock前缀的指令会引起处理器缓存写回内存； 一个处理器的缓存回写到内存会导致其他处理器的缓存失效； 当处理器发现本地缓存失效后，就会从内存中重读该变量数据，即可以获取当前最新值。 volatile的happens-before关系 黑色的代表根据程序顺序规则推导出来，红色的是根据volatile变量的写happens-before 于任意后续对volatile变量的读，而蓝色的就是根据传递性规则推导出来的。 volatile的内存语义 从横向来看，线程A和线程B之间进行了一次通信，线程A在写volatile变量时，实际上就像是给B发送了一个消息告诉线程B你现在的值都是旧的了，然后线程B读这个volatile变量时就像是接收了线程A刚刚发送的消息。 原子性、可见性以及有序性 原子性 原子性是指一个操作是不可中断的，要么全部执行成功要么全部执行失败，有着“同生共死”的感觉。及时在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程所干扰。我们先来看看哪些是原子操作，哪些不是原子操作，有一个直观的印象： int a = 10; //1 a++; //2 int b=a; //3 a = a+1; //4 上面这四个语句中只有第1个语句是原子操作. java内存模型中定义了8中操作都是原子的，不可再分的。 lock(锁定)：作用于主内存中的变量，它把一个变量标识为一个线程独占的状态； unlock(解锁):作用于主内存中的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便后面的load动作使用； load（载入）：作用于工作内存中的变量，它把read操作从主内存中得到的变量值放入工作内存中的变量副本 use（使用）：作用于工作内存中的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作； assign（赋值）：作用于工作内存中的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作； store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送给主内存中以便随后的write操作使用； write（操作）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 （重点）AtomicX类型之所以能实现原子性，是因为其源码中实现了unsafe类，而unsafe类的方法里会使用CAS方法（拿当前值与底层的值进行对比，如果相同则进行swap操作） AtomicInteger类： public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } Unsafe类： public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 附：AtomicLong和LongAdder类： 补充知识：对于64位的long及double类型，jvm允许将64位的读/写操作 拆分成 两次32位的读/写操作。 LongAdder：经过一系列方法确保效率高，高并发时优先使用，但精度可能会下降。 AtomicLong：序列号生成这一类需要准确的数值时，使用AtomicLong Atomic的ABA问题： 线程 1 从内存位置V中取出A。 线程 2 从位置V中取出A。 线程 2 进行了一些操作，将B写入位置V。 线程 2 将A再次写入位置V。 线程 1 进行CAS操作，发现位置V中仍然是A，操作成功。 尽管线程 1 的CAS操作成功，但不代表这个过程没有问题——对于线程 1 ，线程 2 的修改已经丢失。 解决方法：用AtomicStampedReference/AtomicMarkableReference （维护了一个“状态戳”） 可见性 当一个线程修改了共享变量时，另一个线程可以读取到这个修改后的值。 a）synchronize：解锁前，把共享变量写入主内存。加锁时，清空工作内存中共享变量，确保从主内存中读到最新的值。（写对应释放锁，读对应加锁） b）volatile：保证可见性和禁止重排序 保证可见性：volatile写操作会把共享变量写入主内存。volatile读操作会从主内存中读到最新的值，然后放到工作内存中。 禁止重排序：不允许代码段内出现重排序。 ！！！！（注意：对于volatile int count，在多线程环境下count++ 并不能保证线程安全）即volatile不具备原子性！！！！ 不具备的原因：https://blog.csdn.net/xdzhouxin/article/details/81236356 实际上，volatile不适合计数，但很适合作为状态量的标识 有序性 synchronized synchronized语义表示锁在同一时刻只能由一个线程进行获取，当锁被占用后，其他线程只能等待。因此，synchronized语义就要求线程在访问读写共享变量时只能“串行”执行，因此synchronized具有有序性。 volatile 在java内存模型中说过，为了性能优化，编译器和处理器会进行指令重排序；也就是说java程序天然的有序性可以总结为：如果在本线程内观察，所有的操作都是有序的；如果在一个线程观察另一个线程，所有的操作都是无序的。在单例模式的实现上有一种双重检验锁定的方式（Double-checked Locking）。代码如下： public class Singleton { private Singleton() { } private volatile static Singleton instance; public Singleton getInstance(){ if(instance==null){ synchronized (Singleton.class){ if(instance==null){ instance = new Singleton(); } } } return instance; } } 这里为什么要加volatile了？我们先来分析一下不加volatile的情况，有问题的语句是这条： instance = new Singleton(); 这条语句实际上包含了三个操作：1.分配对象的内存空间；2.初始化对象；3.设置instance指向刚分配的内存地址。但由于存在重排序的问题，可能有以下的执行顺序： 如果2和3进行了重排序的话，线程B进行判断if(instance==null)时就会为true，而实际上这个instance并没有初始化成功，显而易见对线程B来说之后的操作就会是错得。而用volatile修饰的话就可以禁止2和3操作重排序，从而避免这种情况。volatile包含禁止指令重排序的语义，其具有有序性。 J.U.C J.U.C之AQS (重要！背）AQS依赖一条同步队列（FIFO，由一个个Node组成，双向链表），且维护一个volatile int的共享资源state。 state=0表示同步状态可用（如果用于锁，则表示锁可用），state=1表示同步状态已被占用（锁被占用）。 private volatile int state 2种同步方式：独占式，共享式。独占式如ReentrantLock，共享式如Semaphore和CountDownLatch 模板方法模式: 　tryAcquire(int arg) : 独占式获取同步状态 　tryRelease(int arg) ：独占式释放同步状态 　tryAcquireShared(int arg) ：共享式获取同步状态 　tryReleaseShared(int arg) ：共享式释放同步状态 CountDownLatch （多线程运行，确保latch中的线程都执行完后其他线程才变成resume状态） ①调用countDown() 让计数减1 ②调用await() 等待计数变成0后，其他线程（本例中为主线程）变成resume。 ③await()可以设置时间，达到时间就接着往下进行。 注意：以下代码中的test方法中都会等待1秒，以便实现线程的堵塞。 CyclicBarrier （多线程计算数据，最后合并结果） 当test方法里使用await()的个数达到5个（即有五个线程ready），这五个同时变成resume。 注：这里的await()方法实际上用的就是Condition.await()； 常用场景：达到个数时回滚，以及多线程计算数据，最后合并结果： 考题：Java中CyclicBarrier 和 CountDownLatch有什么不同？ a）CountDownLatch 对外，CyclicBarrier对内。前者是一组线程都countDown后其他线程才能接着进行。而后者是内部多个线程相互等待，都ready后再一起执行。 b）与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。 Semaphore 调用acquire() 和 release() 方法。先在构造函数中设置好资源数，当资源不够某个线程获取时，便进入堵塞状态。 tryAcquire()：立即尝试获取资源 ，获取不到便丢弃线程并返回false。（下图中只会有3个线程执行，其余的线程因为获取不到资源被丢弃） tryAcquire()还可以带时间参数，表示多久 锁 锁的一些概念： 公平锁，非公平锁： 公平锁：先请求锁的一定先被满足，即FIFO 非公平锁则反之，容易造成一个线程连续获得锁的情况，导致线程“饥饿”。 悲观锁、乐观锁： 悲观锁：一般用于写操作，认为自己取数据的时候总是会有人在修改。再比如Java里面的同步原语synchronized关键字的实现也是悲观锁。 共享锁：一般用于读操作，是一种乐观锁。读的时候不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据（例如CAS），可以使用版本号等机制。 排他锁、共享锁：（适用场景：1. 数据库的锁 2. ReentrantReadWriteLock） 共享锁（读锁）：读可以共享，但是读写之间互斥。 排他锁（写锁）：只有自己能操作，其他人都不能访问。 自旋锁 对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，“自旋”一词就是因此而得名。 偏向锁，轻量级锁，重量级锁：（非重点） 偏向锁（无锁）：顾名思义，它会偏向于第一个访问锁的线程，如果只有一个线程运行，无竞争，那么会给这个线程一个偏向锁，消除这个线程锁重入（CAS）的开销。 轻量级锁（CAS）：轻量级锁是由偏向所升级来的，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁；轻量级锁的意图是在没有多线程竞争的情况下，通过CAS尝试将MarkWord更新为指向LockRecord的指针，减少了使用重量级锁的系统互斥量产生的性能消耗。 重量级锁（Synchronized）：当更新失败时，会检查MarkWord是否指向当前线程的栈帧。如果不是，说明这个锁被其他线程抢占，此时膨胀为重量级锁。 ReentrantLock （重入锁） 重入锁的原理 a）和synchronize的原理一样都是使用计数器 b）ReentrantLock中的Sync实现了AQS，所以实际上是有一条同步队列来进行控制的。 Synchronize和ReentrantLock的区别（初级程序员优先使用synchronize） 性能上： synchronized在资源竞争不是很激烈的情况下是很合适的。原因在于，编译程序通常会尽可能的优化synchronized，另外可读性非常好。 ReentrantLock: 当同步非常激烈的时候，ReentrantLock还能维持常态。 功能上：Synchronize有的ReentrantLock都有。但ReentrantLock有一些独有的功能： a）可指定是公平锁还是非公平锁 （默认是非公平锁） b）提供了Condition类，可分组唤醒线程 c）lock.lockInterruptibly()，允许在等待时由其它线程调用等待线程的Thread.interrupt方法来中断等待线程的等待而直接返回，这时不用获取锁，而会抛出一个InterruptedException。 注：ReentrantLock的锁释放一定要在finally中处理，否则可能会产生严重的后果。 Condition的使用： Condition 实现等待的时候内部是一个等待队列，包含着head节点和tail节点，等待队列中的每一个节点是一个 AbstractQueuedSynchronizer.Node 实例。 Condition 的本质就是等待队列和同步队列的交互： 当一个持有锁的线程调用 Condition.await() 方法，那么该线程会释放锁，然后构造成一个Node节点加入到等待队列的队尾。 当一个持有锁的线程调用 Condition.signal() 时，它会执行以下操作： 将等待队列队首的节点移到同步队列，然后对其进行唤醒操作。 读写锁 ReentrantReadWriterLock （没有读写的情况才能写） 分别有一个readLock和一个writeLock readLock是共享锁，writeLock是排它锁，也就意味着： 1、读和读之间不互斥 （与非读写锁的区别就在这） 2、写和写之间互斥 3、读和写之间互斥（这个超级重要） 读写锁的锁降级概念：（获得读锁的特例） 锁降级：t0 上写锁 --> 修改值 --> 上读锁 --> 释放写锁 --> 使用数据 --> 释放读锁 也就是说，锁降级就是在释放写锁的前一步上读锁，因为是同一个线程，所以这两个锁不会冲突。 如果不使用锁降级，可能会出现以下原因：（重点看1和2） t0 上写锁 --> 修改值 --> 释放写锁 --> 使用数据，即使用写锁后直接使用刚刚修改数据 ，与此同时，t1在t0释放写锁后获得写锁。这样t0使用的是t1修改前的旧数据。 t0 上写锁 --> 修改值 --> 释放写锁 -->上读锁 --> 使用数据，也就是用完写锁再去获取读锁，其缺点在于，如果有别的线程在等待获取写锁，那么上读锁时会进入等待队列进行等待，就无法立即使用刚修改的值。 t0 上写锁 --> 修改值 --> 使用数据 --> 释放写锁，这样做虽然不会有问题，但这样是以排它锁的方式使用读写锁，就没有了使用读写锁的优势。 读锁不互斥，写锁之间的互斥的，但读写之间是互斥的。 LockSupport： LockSupport定义了一组以park开头的方法来阻塞当前线程，以及unpark方法来唤醒一个被阻塞的线程。 线程池 为什么用线程池? 1.创建/销毁线程伴随着系统开销，过于频繁的创建/销毁线程，会很大程度上影响处理效率。（线程复用） 2.线程并发数量过多，抢占系统资源从而导致阻塞。（控制并发数量） 3.对线程进行一些简单的管理。（管理线程） 线程池的原理 （1）线程复用：实现线程复用的原理应该就是要保持线程处于存活状态（就绪，运行或阻塞） （2）控制并发数量：（核心线程和最大线程数控制） （3）管理线程（设置线程的状态） 线程池的参数： corePoolSize：核心线程数 maximumPoolSize：最大线程数(一般设置为INTMAX) keepAliveSeconds:空闲存活时间 （在corePoreSizeworkQueue：阻塞队列，用来保存等待被执行的任务 当一个任务被添加进线程池时，执行策略： 线程数量未达到corePoolSize，则新建一个线程(核心线程)执行任务 线程数量闲达到了corePoolSize，则将任务移入队列，等待空线程将其取出去执行 （通过getTask()方法从阻塞队列中获取等待的任务，如果队列中没有任务，getTask方法会被阻塞并挂起，不会占用cpu资源，整个getTask操作在自旋下完成） 队列已满，新建线程(非核心线程)执行任务 队列已满，总线程数又达到了maximumPoolSize，就会执行任务拒绝策略。 线程池的任务拒绝策略 ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 常见四种线程池： 可缓存线程池CachedThreadPool() public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } 根据源码可以看出： 这种线程池内部没有核心线程，线程的数量是有没限制的。 在创建任务时，若有空闲的线程时则复用空闲的线程，若没有则新建线程。 没有工作的线程（闲置状态）在超过了60S还不做事，就会销毁。 适用：执行很多短期异步的小程序或者负载较轻的服务器。 FixedThreadPool 定长线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } 根据源码可以看出： 该线程池的最大线程数等于核心线程数，所以在默认情况下，该线程池的线程不会因为闲置状态超时而被销毁。 如果当前线程数小于核心线程数，并且也有闲置线程的时候提交了任务，这时也不会去复用之前的闲置线程，会创建新的线程去执行任务。如果当前执行任务数大于了核心线程数，大于的部分就会进入队列等待。等着有闲置的线程来执行这个任务。 适用：执行长期的任务，性能好很多。 SingleThreadPool public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())); } 根据源码可以看出： 有且仅有一个工作线程执行任务，所有任务按照指定顺序执行，即遵循FIFO规则。 适用：一个任务一个任务执行的场景。 ScheduledThreadPool public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } //ScheduledThreadPoolExecutor(): public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue()); } 根据源码可以看出： DEFAULT_KEEPALIVE_MILLIS就是默认10L，这里就是10秒。这个线程池有点像是CachedThreadPool和FixedThreadPool 结合了一下。 不仅设置了核心线程数，最大线程数也是Integer.MAX_VALUE。 这个线程池是上述4个中唯一一个有延迟执行和周期执行任务的线程池。 适用：周期性执行任务的场景（定期的同步数据） 总结：除了new ScheduledThreadPool 的内部实现特殊一点之外，其它线程池内部都是基于ThreadPoolExecutor类实现的。 在ThreadPoolExecutor类中有几个非常重要的方法： execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()，这个方法也是用来向线程池提交任务的，实际上它还是调用的execute()方法，只不过它利用了Future来获取任务执行结果。 shutdown()不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务。 shutdownNow(）立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务。 线程池中的最大线程数 一般说来，线程池的大小经验值应该这样设置：（其中N为CPU的个数） 如果是CPU密集型应用（CPU使用频率高，不适合频繁切换），则线程池大小设置为N+1 如果是IO密集型应用，则线程池大小设置为2N+1 ThreadLocal 一句话：每个线程内部都有一个ThreadLocalMap，map中key为ThreadLocal自己（this），value为任意对象。对于不同的线程，threadLocal都是一样的，但value都是不同的。因此可以适合用来做线程数据隔离。 原理 ThreadLocal类提供的几个方法： get() 方法 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; } return setInitialValue(); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } remove()方法 (略） 初始容量16，负载因子2/3，解决冲突的方法是再hash法，也就是：在当前hash的基础上再自增一个常量进行哈希。 应用场景 最常见的ThreadLocal使用场景为 用来解决数据库连接、Session管理等。 private static final ThreadLocal threadLocal = new ThreadLocal(); public static Session getSession() throws InfrastructureException { Session s = (Session) threadLocal.get(); try { if (s == null) { s = getSessionFactory().openSession(); threadLocal.set(s); } } catch (HibernateException ex) { throw new InfrastructureException(ex); } return s; 并发容器 注意：以下队列的共性： 1.put()和take()是会阻塞的，而offer()和poll()是不会阻塞的。 2.并发都是通过ReentrantLock实现的。 3.阻塞是通过两个condition实现的（notEmpty和notFull）。 阻塞队列：BlockingQueue 这里blocking的具体含义： 当队列中没有数据的情况下，消费者端会被自动阻塞（挂起），直到有数据放入队列。 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒。 ArrayBlockingQueue 读和写共用同一个ReentrantLock，也就意味着（读读、写写、读写都互斥）。 并发是由ReentrantLock来实现，而阻塞是有lock的两个Condition来实现。 //仍然是有数组实现 final Object[] items; //并发是由ReentrantLock来控制 final ReentrantLock lock; //阻塞是有两个Condition来实现 private final Condition notEmpty; private final Condition notFull; //put方法会先加锁，操作完再解锁，如果阻塞进入await() public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) notFull.await(); //enqueue里有signal操作 enqueue(e); } finally { lock.unlock(); } } public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); //dequeue里有signal操作 return dequeue(); } finally { lock.unlock(); } } 错误:在线程1读空队列阻塞，然后牢牢占据着锁不放 -> 线程2尝试写到该队列会发生死锁。 原因在于如果进入阻塞（await）会先把锁释放！并不会占据着锁不放！ LinkedBlockingQueue 生产者端和消费者端分别采用了独立的锁来控制数据同步，也就意味着读和写之间是不互斥的！（注意这里和读写锁ReentrantReadWriteLock的区别）。 同样，阻塞是通过两个Condition实现的。 //内部使用带next的Node来实现linked结构 static class Node { E item; Node next; Node(E x) { item = x; } } //读锁 private final ReentrantLock takeLock = new ReentrantLock(); //读阻塞 private final Condition notEmpty = takeLock.newCondition(); // 写锁 private final ReentrantLock putLock = new ReentrantLock(); //写阻塞 private final Condition notFull = putLock.newCondition(); PriorityBlockingQueue 只有一个锁，内部控制线程同步的锁采用的是公平锁。queue中存储的类需要实现compare方法。 LinkedBlockingDeque 双向队列，只有一个锁，两个condition。 DelayQueue:延时获取 除此之外还有三种队列。 ConcurrentHashMap CocurrentHashMap（JDK 1.7）： CocurrentHashMap 是由 Segment 数组和 HashEntry 数组和链表组成。 Segment：一个ReentrantLock,也就是说segment扮演者锁的角色。 核心数据如 value，以及链表都是 volatile 修饰的，保证了获取时的可见性 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理 ConcurrentHashMap会对数据hash后 对摘要值进行二次hash，其目的是减少hash冲突，使元素均匀分布。 ConcurrentHashMap（JDK 1.8） （读不加锁、写加锁） JDK1.8 的实现已经摒弃了Segment的概念，恢复成HashMap的结构（bucket数组+链表+红黑树），并发控制使用 Synchronized 和 CAS 来操作，整个看起来就像是优化过且线程安全的HashMap. JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry。 JDK1.8使用内置锁synchronized来代替重入锁ReentrantLock 因为JDK1.8中，hashMap引入了红黑树，所以cocurrentHashMap也同样使用了红黑树。 CocurrentHashMap（1.8）中get操作为什么不加锁？ get操作可以无锁是由于Node的元素val和指针next是用volatile修饰的，这样可保证多线程对node的可见性。 CocurrentHashMap（1.8）中put操作为什么加锁？ 正因为node中元素是用volatile修饰，并不能保证原子性，所以写操作需要加锁来保证原子性。 为什么使用ConcurrentHashMap而不使用hashtable？ 它们都可以用于多线程的环境，但是当Hashtable的大小增加到一定的时候，性能会急剧下降，因为迭代时需要被锁定很长的时间。而ConcurrentHashMap的锁粒度是HashEntry，即便数组长度很长，也能保持比较好的性能。 源码解析： table初始化： table初始化操作会延缓到第一次put行为。但是put是可以并发执行的，所以初始化还是要考虑并发。 sizeCtl默认为0，非默认sizeCtl会是一个2的幂次方的值（详见hashMap）。 具体并发控制操作请看代码注释： private final Node[] initTable() { Node[] tab; int sc; while ((tab = table) == null || tab.length == 0) { //如果一个线程发现sizeCtl 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = tab = nt; sc = n - (n >>> 2); } } finally { sizeCtl = sc; } break; } } return tab; } put操作： put操作采用CAS+synchronize来控制 其中table中某一格为空，也就是第一次插入时采用CAS，而之后链表或者红黑树的插入就采用synchronize final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); //关键在这一块！CAS操作的条件！ else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) { if (casTabAt(tab, i, null, new Node(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //下面使用synchronize来插入链表或红黑树（代码略） } addCount(1L, binCount); return null; } ConcurrentLinkedQueue：（了解） 是一个单向队列，队列由Node组成 并发的确保：head和tail设置为volatile，Node中的item和next也设置为volatile。 private transient volatile Node head; private transient volatile Node tail; private static class Node { volatile E item; volatile Node next; ... Node中操作节点数据的API，都是通过Unsafe机制的CAS函数实现的；例如casNext()是通过CAS函数“比较并设置节点的下一个节点” 补充题目 你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写。 关键：使用ReentrantReadWriteLock class MyData{ //数据 private static String data = \"0\"; //读写锁 private static ReadWriteLock rw = new ReentrantReadWriteLock(); //读数据 public static void read(){ rw.readLock().lock(); System.out.println(Thread.currentThread()+\"读取一次数据：\"+data+\"时间：\"+new Date()); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } finally { rw.readLock().unlock(); } } //写数据 public static void write(String data){ rw.writeLock().lock(); System.out.println(Thread.currentThread()+\"对数据进行修改一次：\"+data+\"时间：\"+new Date()); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } finally { rw.writeLock().unlock(); } } } 你将如何使用thread dump？你将如何分析Thread dump？ kill -3 说明： pid： Java 应用的进程 id ,也就是需要抓取 dump 文件的应用进程 id 。 当使用 kill -3 生成 dump 文件时，dump 文件会被输出到标准错误流。假如你的应用运行在 tomcat 上，dump 内容将被发送到/logs/catalina.out 文件里。 dump文件示例： \"pool-1-thread-13\" prio=6 tid=0x000000000729a000 nid=0x2fb4 runnable [0x0000000007f0f000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.read(SocketInputStream.java:129) at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264) at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158) - locked (a java.io.InputStreamReader) at java.io.InputStreamReader.read(InputStreamReader.java:167) at java.io.BufferedReader.fill(BufferedReader.java:136) at java.io.BufferedReader.readLine(BufferedReader.java:299) - locked (a java.io.InputStreamReader) at java.io.BufferedReader.readLine(BufferedReader.java:362) * 线程名称：pool-1-thread-13 * jvm线程id：tid=0x000000000729a000 * 线程状态：runnable * 起始栈地址：[0x0000000007f0f000] 用java写一个死锁 public static void main(String[] args) { Object lock1 = new Object(); Object lock2 = new Object(); ExecutorService exec = Executors.newCachedThreadPool(); exec.execute(() ->{ synchronized(lock1) { System.out.println(\"get lock1,want lock2...\"); Thread.sleep(1000); //try-catch忽略 synchronized (lock2) { System.out.println(\"get lock2\"); } } } ); exec.execute(()->{ synchronized (lock2) { System.out.println(\"get lock2,want lock1....\"); Thread.sleep(1000); //try-catch忽略 synchronized (lock1) { System.out.println(\"get lock1\"); } } }); } 使用notFull和notEmpty来实现生产者/消费者模型 public class Depot { ReentrantLock lock = new ReentrantLock(); Condition notFull = lock.newCondition(); Condition notEmpty = lock.newCondition(); LinkedList queue; int limit; public Depot(int limit) { queue = new LinkedList<>(); this.limit = limit; } public void produce(int i) { lock.lock(); try { // System.out.println(\"生产\" + i); //满了阻塞 if(queue.size() == limit) // { // System.out.println(\"队列已满，进入阻塞\"); notFull.await(); // System.out.println(i+ \"已被唤醒\"); // } queue.offer(i); notEmpty.signal(); }catch(Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public int consume() { int num = -1; lock.lock(); try { //空了阻塞 if(queue.size() == 0) // { // System.out.println(\"队列已空，进入阻塞\"); notEmpty.await(); // System.out.println(\"已被唤醒\"); // } num = queue.poll(); // System.out.println(\"消费：\"+num); notFull.signal(); }catch(Exception e) { e.printStackTrace(); } finally { lock.unlock(); } return num; } } public static void main(String[] args) { ExecutorService exec = Executors.newCachedThreadPool(); Depot depot = new Depot(5); for(int i = 0; i { depot.produce(num); }); } try { Thread.sleep(200); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } for(int i = 0; i { depot.consume(); }); } try { Thread.sleep(200); }catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } for(int i = 0; i { depot.produce(num); }); } } 3个任务，返回结果，如果超时200ms，返回空： //调用三个任务，要求200ms内返回结果或者空 public class 两百秒内返回结果或者空 { public static void main(String[] args) { ExecutorService exec = Executors.newCachedThreadPool(); Future future1 = exec.submit(new MyTask()); Integer i1 = future1.get(200,TimeUnit.MILLISECONDS); if(i1 == null) System.out.println(\"null\"); else System.out.println(i1); } } class MyTask implements Callable { @Override public Integer call() throws Exception { int sum = 0; for(int i = 0;i Copyright © 程潇 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-24 21:08:13 "},"Chapter1/Redis.html":{"url":"Chapter1/Redis.html","title":"Redis","keywords":"","body":"Redis redis运维日志 安装 $ wget http://download.redis.io/releases/redis-3.2.11.tar.gz $ tar -zxvf redis-3.2.11.tar.gz $ ln -s redis-3.2.11 redis $ cd redis $ make & make install 启动方式 直接启动：redis-server 动态参数启动：redis-server -p 6380 指定配置文件启动：redis-server /path/to/redis-**.conf 推荐基础配置： # 是否以守护进程方式启动 daemonize yes # redis对外端口 port 6380 # 工作目录 dir ./ # 日志文件 logfile \"redis-6380.log\" redis客户端 redis-cli -h ip -p port redis API 通用命令 keys [re pattern]:显示满足通配符匹配的key dbsize：查看key总数 exist key：判断key是否存在 del key [key...]：删除指定key-value expire key seconds：设置key过期时间 ttl key：查看key剩余过期时间 persist key：删除key的过期设置 127.0.0.1:6379> set hello world OK 127.0.0.1:6379> expire hello 20 (integer) 1 127.0.0.1:6379> ttl hello (integer) 16 127.0.0.1:6379> get hello \"world\" 127.0.0.1:6379> ttl hello (integer) 7 127.0.0.1:6379> ttl hello (integer) -2 (-2代表key已经不存在了) 127.0.0.1:6379> get hello (nil) 127.0.0.1:6379> set hello world OK 127.0.0.1:6379> expire hello 20 (integer) 1 127.0.0.1:6379> ttl hello (integer) 16 (还有16秒过期) 127.0.0.1:6379> persist hello (integer) 1 127.0.0.1:6379> ttl hello (integer) -1 (-1代表key存在，并且没有设置过期时间) 127.0.0.1:6379> get hello \"wordl\" type key:查看key对应value的数据类型（string hash list set zset） 通用命令的时间复杂度 数据结构和内部编码 单线程 redis是单线程设计的，使用时应该注意以下几点： 一次只运行一条命令 不要执行长（慢）命令 长（慢）命令：keys, flushall, flushdb, slow lua script, mutil/exec, operate big value(collection)都是>=O（n）复杂度的命令 其实redis也不全是单线程，比如异步生成rdb文件等 Redis基本数据类型 字符串 字符串常用命令 时间复杂度 使用场景 页面动态缓存 比如生成一个动态页面，首次可以将后台数据生成页面，并且存储到redis 字符串中。再次访问，不再进行数据库请求，直接从redis中读取该页面。特点是：首次访问比较慢，后续访问快速。 数据缓存 在前后分离式开发中，有些数据虽然存储在数据库，但是更改特别少。比如有个全国地区表。当前端发起请求后，后台如果每次都从关系型数据库读取，会影响网站整体性能。 我们可以在第一次访问的时候，将所有地区信息存储到redis字符串中，再次请求，直接从数据库中读取地区的json字符串，返回给前端。 数据统计 redis整型可以用来记录网站访问量，某个文件的下载量。（自增自减） 时间内限制请求次数 比如已登录用户请求短信验证码，验证码在5分钟内有效的场景。 当用户首次请求了短信接口，将用户id存储到redis 已经发送短信的字符串中，并且设置过期时间为5分钟。当该用户再次请求短信接口，发现已经存在该用户发送短信记录，则不再发送短信。 分布式session 当我们用nginx做负载均衡的时候，如果我们每个从服务器上都各自存储自己的session，那么当切换了服务器后，session信息会由于不共享而会丢失，我们不得不考虑第三应用来存储session。通过我们用关系型数据库或者redis等非关系型数据库。关系型数据库存储和读取性能远远无法跟redis等非关系型数据库。 Redis字符串常用命令以及应用场景 深入解析Redis中常见的应用场景 哈希 哈希常用命令 时间复杂度 使用场景 满足key-field-value的数据结构类型的，且value变动频繁，例如： 购物车 缓存视频的基本信息 等等 列表 列表常用命令 时间复杂度 使用场景 Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。 List 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用List结构，我们可以轻松地实现最新消息排行等功能。List的另一个应用就是消息队列， 可以利用List的PUSH操作，将任务存在List中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作List中某一段的api，你可以直接查询，删除List中某一段的元素。 栈：LPUSH + LPOP 队列： LPUSH + RPOP 定长集合：LPUSH + LTRIM 消息队列：LPUSH + BRPOP 集合 集合常用命令 时间复杂度 使用场景 集合有取交集、并集、差集等操作，因此可以求共同好友、共同兴趣、分类标签等。 1、标签：比如我们博客网站常常使用到的兴趣标签，把一个个有着相同爱好，关注类似内容的用户利用一个标签把他们进行归并。 2、共同好友功能，共同喜好，或者可以引申到二度好友之类的扩展应用。 3、统计网站的独立IP。利用set集合当中元素不唯一性，可以快速实时统计访问网站的独立IP。 有序集合 有序集合常用命令 时间复杂度 使用场景 排行榜系统 有序集合比较典型的使用场景就是排行榜系统，例如视频网站需要对用户上传的视频做排行榜，榜单的维度可能是多个方面的：按照时间、按照播放数量、按照获得的赞数。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 Redis其他功能 慢查询 生命周期 配置和命令 配置 slowlog-max-len：慢查询队列长度 slowlog-log-slower-than：慢查询阈值（单位：微秒） slowlog-log-slower-than=0, 记录所有命令 slowlog-log-slower-than 配置方法 默认值 config get slowlog-max-len = 128 config get slowlog-log-slower-than = 10000 动态配置 config set slowlog-max-len 1000 config set slowlog-log-slower-than 1000 命令 slowlog get [n]：获取慢查询队列 slowlog len：获取慢查询队列长度 slowlog reset：清空慢查询队列 运维经验 slowlog-max-len不要设置过大，默认10ms，通常设置1ms\u000b Redis的QPS是万级的，也就是一条命令平均0.1ms就执行结束了。通常我们翻10倍，设置1ms。 slowlog-log-slower-than不要设置过小，通常设置1000左右\u000b 慢查询队列默认长度128，超过的话，按先进先出，之前的慢查询会丢失。通常我们设置1000。 理解命令生命周期\u000b 慢查询发生在第三阶段 定期持久化慢查询\u000b slowlog get或其他第三方开源工具 pipeline 什么是流水线 未使用流水线的网络通信模型： 假设客户端在上海，Redis服务器在北京。相距1300公里。假设光纤速度≈光速2/3，即30000公里/秒2/3。那么一次命令的执行时间就是(13002)/(300002/3)=13毫秒。Redis万级QPS，一次命令执行时间只有0.1毫秒，因此网络传输消耗13毫秒是不能接受的。在N次命令操作下，Redis的使用效率就不是很高了。 使用pipeline的网络通信模型： 客户端实现 引入maven依赖： redis.clients jedis 2.9.0 jar compile 客户端： // 不用pipeline Jedis jedis = new Jedis(\"127.0.0.1\", 6379); for (int i = 0; i 使用pipelne，10000次hset，总共耗时0.7s，不同网络环境可能有所不同。 可见在执行批量命令时，使用pipeline对Redis的使用效率提升是非常明显的。 与M原生操作对比 mset、mget等操作是原子性操作，一次m操作只返回一次结果。pipeline非原子性操作，只是将N次命令打个包传输，最终命令会被逐条执行，客户端接收N次返回结果。 pipeline使用建议 注意每次pipeline携带数据量,pipeline主要就是压缩N次操作的网络时间。但是pipeline的命令条数也不建议过大，避免单次传输数据量过大，客户端等待过长。 Redis集群中，pipeline每次只作用在一个Reids节点上。 发布订阅 角色 发布者（publisher） 订阅者（subscriber） 频道（channel） 模型 API 发布 API：publish channel message redis> publish sohu:tv \"hello world\" (integer) 3 #订阅者个数 redis> publish sohu:auto \"taxi\" (integer) #没有订阅者 订阅 API：subscribe [channel] #一个或多个 redis> subscribe sohu:tv 1) \"subscribe\" 2) \"sohu:tv\" 3) (integer) 1 1) \"message\" 2) \"sohu:tv\" 3) \"hello world\" 取消订阅 API：unsubscribe [channel] #一个或多个 redis> unsubscribe sohu:tv 1) \"unsubscribe\" 2) \"sohu:tv\" 3) (integer) 0 其他API psubscribe [pattern…] #订阅指定模式 punsubscribe [pattern…] #退订指定模式 pubsub channels #列出至少有一个订阅者的频道 pubsub numsubs [channel…] #列出给定频道的订阅者数量 对比消息队列 发布订阅模型，订阅者均能收到消息。消息队列，只有一个订阅者能收到消息。因此使用发布订阅还是消息队列，要搞清楚使用场景。 GEO geo是什么 GEO：存储经纬度，计算两地距离，范围计算等 相关命令 API：geoadd key longitude latitude member # 增加地理位置信息 redis> geoadd cities:locations 116.28 39.55 bejing (integer) 1 redis> geoadd cities:locations 117.12 39.08 tianjin 114.29 38.02 shijiazhuang 118.01 39.38 tangshan 115.29 38.51 baoding (integer) 4 API：geopos key member [member…] # 获取地理位置信息 redis> geopos cities:locations tianjin 1)1) \"117.12000042200088501\" 2) \"39.0800000535766543\" API：geodist key member1 member2 [unit] # 获取两位置距离,unit:m(米)、km(千米)、mi(英里)、ft(尺) reids> geodist cities:locations tianjin beijing km \"89.2061\" API： 获取指定位置范围内的地理位置信息集合 georadius key longitude latitude radius m|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] georadiusbymember key member radius m|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] withcoord: 返回结果中包含经纬度\u000b withdist: 返回结果中包含距离中心节点的距离\u000b withhash: 返回结果中包含geohash\u000b COUNT count：指定返回结果的数量\u000b asc|desc：返回结果按照距离中心节点的距离做升序/降序\u000b store key：将返回结果的地理位置信息保存到指定键\u000b storedist key：将返回结果距离中心点的距离保存到指定键 redis> georadiusbymember cities:locations beijing 150 km 1）\"beijing\" 2) \"tianjin\" 3) \"tangshan\" 4) \"baoding\" 相关说明 since redis3.2+ redis geo是用zset实现的，type geokey = zset 没有删除API，可以使用zset的API：zrem key member Redis持久化 什么是持久化？Redis的数据操作都在内存中，redis崩掉的话，会丢失。Redis持久化就是对数据的更新异步的保存在磁盘上，以便数据恢复。 实现方式 快照方式（RDB） 写日志方式（AOF） RDB 什么是RDB 将Redis内存中的数据，完整的生成一个快照，以二进制格式文件（后缀RDB）保存在硬盘当中。当需要进行恢复时，再从硬盘加载到内存中。 Redis主从复制，用的也是基于RDB方式，做一个复制文件的传输。 触发方式 save命令触发方式（同步） redis> save OK save执行时，会造成Redis的阻塞。所有数据操作命令都要排队等待它完成。 文件策略：新生成一个新的临时文件，当save执行完后，用新的替换老的。 bgsave命令触发方式（异步） redis> bgsave Background saving started 客户端对Redis服务器下达bgsave命令时，Redis会fork出一个子进程进行RDB文件的生成。当RDB生成完毕后，子进程再反馈给主进程。fork子进程时也会阻塞，不过正常情况下fork过程都非常快的。 文件策略：与save命令相同。 配置触发方式(不建议，RDB生成会很频繁) 修改配置文件： # 配置自动生成规则。一般不建议配置自动生成RDB文件 save 900 1 save 300 10 save 60 10000 # 指定rdb文件名 dbfilename dump-${port}.rdb # 指定rdb文件目录 dir /opt/redis/data # bgsave发生错误，停止写入 stop-writes-on-bgsave-error yes # rdb文件采用压缩格式 rdbcompression yes # 对rdb文件进行校验 rdbchecksum yes 不容忽略的触发方式 全量复制 主从复制时，主会自动生成RDB文件。 debug reload Redis中的debug reload提供debug级别的重启，不清空内存的一种重启，这种方式也会触发RDB文件的生成。 shutdown 会触发RDB文件的生成。 AOF 就是写日志，每次执行Redis写命令，让命令同时记录日志（以AOF日志格式）。Redis宕机时，只要进行日志回放就可以恢复数据。 RDB存在的问题 耗时、耗内存、耗IO性能 将内存中的数据全部dump到硬盘当中，耗时。bgsave的方式fork()子进程耗额外内存。大量的硬盘读写耗费IO性能。(IO密集型) 不可控、丢失数据 宕机时，上次快照之后写入的内存数据，将会丢失。 AOF三种策略 首先Redis执行写命令，将命令刷新到硬盘缓冲区当中。 always always策略让缓冲区中的数据即时刷新到硬盘。 everysec everysec策略让缓冲区中的数据每秒刷新到硬盘。相比always，在高写入量的情况下，可以保护硬盘。出现故障可能会丢失一秒数据。 no 刷新策略让操作系统来决定。 通常使用everysec策略，这也是AOF的默认策略。 AOF重写 随着时间的推移，命令的逐步写入。AOF文件也会逐渐变大。当我们用AOF来恢复时会很慢，而且当文件无限增大时，对硬盘的管理，对写入的速度也会有产生影响。Redis当然考虑到这个问题，所以就有了AOF重写。 AOF重写就是把过期的、没用的、重复的以及可优化的命令，进行化简。只取最终有价值的结果。虽然写入操作很频繁，但系统定义的key的量是相对有限的。 AOF重写可以大大压缩最终日志文件的大小。从而减少磁盘占用量，加快数据恢复速度。比如我们有个计数的服务，有很多自增的操作，比如有一个key自增到1个亿，对AOF文件来说就是一亿次incr。AOF重写就只用记1条记录。 AOF重写两种方式 bgrewriteaof命令触发AOF重写\u000b redis客户端向Redis发bgrewriteaof命令，redis服务端fork一个子进程去完成AOF重写。这里的AOF重写，是将Redis内存中的数据进行一次回溯，回溯成AOF文件。而不是重写AOF文件生成新的AOF文件去替换。 AOF重写配置\u000b auto-aof-rewrite-min-size：AOF文件重写需要的尺寸\u000b auto-aof-rewrite-percentage：AOF文件增长率\u000b redis提供了aof_current_size和aof_base_size，分别用来统计AOF当前尺寸（单位：字节）和AOF上次启动和重写的尺寸（单位：字节）。\u000b AOF自动重写的触发时机，同时满足以下两点）： aof_current_size > auto-aof-rewrite-min-size aof_current_size – aof_base_size/aof_base_size > auto-aof-rewrite-percentage AOF重写配置 修改配置文件： # 开启正常AOF的append刷盘操作 appendonly yes # AOF文件名 appendfilename \"appendonly-6379.aof\" # 每秒刷盘 appendfsync everysec # 文件目录 dir /opt/soft/redis/data # AOF重写增长率 auto-aof-rewrite-percentage 100 # AOF重写最小尺寸 auto-aof-rewrite-min-size 64mb # AOF重写期间是否暂停append操作。AOF重写非常消耗磁盘性能，而正常的AOF过程中也会往磁盘刷数据。 # 通常偏向考虑性能，设为yes。万一重写失败了，这期间正常AOF的数据会丢失，因为我们选择了重写期间放弃了正常AOF刷盘。 no-appendfsync-on-rewrite yes RDB和AOF的比较 RDB最佳策略 建议关闭RDB 无论是Redis主节点，还是从节点，都建议关掉RDB。但是关掉不是绝对的，主从复制时还是会借助RDB。 用作数据备份 RDB虽然是很重的操作，但是对数据备份很有作用。文件大小比较小，可以按天或按小时进行数据备份。 主从，从开？ 在极个别的场景下，需要在从节点开RDB，可以再本地保存这样子的一个历史的RDB文件。虽然从节点不进行读写，但是Redis往往单机多部署，由于RDB是个很重的操作，所以还是会对CPU、硬盘和内存造成一定影响。根据实际需求进行设定。 AOF最佳策略 建议开启AOF 如果Redis数据只是用作数据源的缓存，并且缓存丢失后从数据源重新加载不会对数据源造成太大压力，这种情况下。AOF可以关。 AOF重写集中管理 单机多部署情况下，发生大量fork可能会内存爆满。 everysec 建议采用每秒刷盘策略 最佳策略 小分片 使用maxmemary对Redis最大内存进行规划。 缓存和存储 根据缓存和存储的特性来决定使用哪种策略 监控（硬盘、内存、负载、网络） 足够的内存 不要把就机器全部的内存规划给Redis。不然会出很多问题。像客户端缓冲区等，不受maxmemary限制。规划不当可能会产生SWAP、OOM等问题。 Redis和memcached 两者都是非关系型内存键值数据库，主要有以下不同： 数据类型 Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。 数据持久化 Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。 分布式 Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 缓存优化 缓存收益与成本的问题 关于缓存收益与成本主要分为三个方面的讲解，第一个是什么是收益；第二个是什么是成本；第三个是有哪些使用场景。 收益 主要有以下两大收益。 加速读写：通过缓存加速读写，如 CPU L1/L2/L3 的缓存、Linux Page Cache 的读写、游览器缓存、Ehchache 缓存数据库结果。 降低后端负载：后端服务器通过前端缓存来降低负载，业务端使用 Redis 来降低后端 MySQL 等数据库的负载。 成本 产生的成本主要有以下三项。 数据不一致：这是因为缓存层和数据层有时间窗口是不一致的，这和更新策略有关的。 代码维护成本：这里多了一层缓存逻辑，就会增加成本。 运维费用的成本：如 Redis Cluster，甚至是现在最流行的各种云，都是成本。 使用场景 使用场景主要有以下三种。 降低后端负载：这是对高消耗的 SQL，join 结果集和分组统计结果缓存。 加速请求响应：这是利用 Redis 或者 Memcache 优化 IO 响应时间。 大量写合并为批量写：比如一些计数器先 Redis 累加以后再批量写入数据库。 数据淘汰策略 键的过期时间 Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ 淘汰策略 可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Redis 具体有 7 种淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）. allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 主动更新 一致性最好的就是主动更新。能够根据代码实时的更新数据，但是维护成本也是最高的；算法剔除和超时剔除一致性都做的不够好，但是维护成本却非常低。 根据需求： 低一致性：最大内存和淘汰策略，数据库有些数据是不需要马上更新的，这个时候就可以用低一致性来操作。 高一致性：超时剔除和主动更新的结合，最大内存和淘汰策略兜底。你没办法确保内存不会增加，从而使服务不可用了。 遇到的问题 缓存穿透问题 如果cache和storage都没有id，依然不断查询，每次查询的cache都会穿透。当请求发送给服务器的时候，缓存找不到，然后都堆到数据库里。这个时候，缓存相当于穿透了，不起作用了。 原因有两点： 业务代码自身的问题。很多实际开发的时候，如果是一个不熟练的程序员，由于缺乏必要的大数据的意识，很多代码在第一次写的时候是 OK 的，但是当需要修改业务代码的时候，常常会出现问题。 恶意攻击和爬虫问题。网络上充斥着各种攻击和各种爬虫模仿着人为请求来访问你的数据。如果恶意访问穿透你的数据库，将会导致你的服务器瞬间产生大量的请求导致服务中止。 那我们去如何发现这些问题呢？ 业务的响应时间：一般请求的时间都是稳定的，但是如果出现类似穿透现象，必然在短时间内有一个体现。 业务本身的问题。产品的功能出现问题。 总调用数，对缓存层命中数、存储层的命中数这些值的采集。 解决方案1：缓存空对象 当缓存中不存在，访问数据库的时候，又找不到数据，需要设置给 cache 的值为 null，这样下次再次访问该 id 的时候，就会直接访问缓存中的 null 了。 但是可能存在的两个问题。首先是需要更多的键，但是如果这个量非常大的话，对业务也是有影响的，所以需要设置过期时间；其次是缓存层和存储层数据“短期”不一致。当缓存层过期时间到了以后，可能会产生和存储层数据不一致的情况。这个时候需要使用一些消息队列等方式，来确保这个值的一致性。 下面的代码用 Java 来实现简单的缓存空对象 public String getCacheThrough(String key){ String cacheValue = cache.get(key); if(StringUtils.isBlank(cacheValue)){ // 如存储数据为空 String storageValue = storage.get(key); cache.set(key,storageValue); if(StringUtils.isBlank(strageValue){ cache.expire(key.60*10);//需要设置一个过期时间 } return storageValue; }else{ return cacheValue; } } 解决方案2：布隆过滤器拦截 布隆过滤器，实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 类似于一个字典，你查词典的时候不需要把所有单词都翻一遍，而是通过目录的方式，用户通过检索的形式在极小内存中可以找到对应的内容。 虽然布隆过滤器可以通过极小的内存来存储，但是免不了需要一部分代码来维护这个布隆过滤器，并且经常需要根据规则来调整，在选取是否使用布隆过滤器，还需要通过场景来选取。 缓存雪崩 缓存雪崩，是指在某一个时间段，缓存集中过期失效，所有的查询都会落到数据库上。 比如在写本文的时候，马上就要到双十二零点，这波商品比较集中的放入了缓存，假设缓存一个小时。那么到了凌晨一点钟的时候，这批商品的缓存就都过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。 解决方案： 解决方案分为三步： 事前（预防）： 选择合适的内存淘汰策略。 对不同分类商品，缓存不同周期。同时，对同一分类中的商品，加上一个随机因子。这样能尽可能分散缓存过期时间。 事中（已经发生）：做二级缓存，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期+限流（实例：本地ehcache缓存 + hystrix限流&降级，避免MySQL崩掉） 事后（已经导致redis宕机）：利用 redis持久化机制保存的数据尽快恢复缓存。 热点 Key问题 我们知道，使用缓存，如果获取不到，才会去数据库里获取。但是如果是热点 key，访问量非常的大，数据库在重建缓存的时候，会出现很多线程同时重建的情况。 解决方法： 互斥锁 由下图所示，第一次获取缓存的时候，加一个锁，然后查询数据库，接着是重建缓存。这个时候，另外一个请求又过来获取缓存，发现有个锁，这个时候就去等待，之后都是一次等待的过程，直到重建完成以后，锁解除后再次获取缓存命中。 但是互斥锁也有一定的问题，就是大量线程在等待的问题。下面我们就来讲一下永远不过期。 永远不过期 首先在缓存层面，并没有设置过期时间（过期时间使用 expire 命令）。但是功能层面，我们为每个 value 添加逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。这样的好处就是不需要线程的等待过程。见下图。 public String getKey(final String key){ V v = redis.get(key); String value = v.getValue(); long logicTimeout = v.getLogicTimeout(); if(logicTimeout 互斥锁的优点是思路非常简单，具有一致性，其缺点是代码复杂度高，存在死锁的可能性。 永不过期的优点是基本杜绝 key 的重建问题，但缺点是不保证一致性，逻辑过期时间增加了维护成本和内存成本。 redis单线程 redis集群的每个节点里只有一个线程负责接受和执行所有客户端发送的请求。技术上使用多路复用I/O，使用Linux的epoll函数，这样一个线程就可以管理很多socket连接。 除此之外，选择单线程还有以下这些原因： 1、redis都是对内存的操作，速度极快（10W+QPS） 2、整体的时间主要都是消耗在了网络的传输上 3、如果使用了多线程，则需要多线程同步，这样实现起来会变的复杂 4、线程的加锁时间甚至都超过了对内存操作的时间 5、多线程上下文频繁的切换需要消耗更多的CPU时间 6、还有就是单线程天然支持原子操作，而且单线程的代码写起来更简单 redis事务 事务大家都知道，就是把多个操作捆绑在一起，要么都执行（成功了），要么一个也不执行（回滚了）。redis也是支持事务的，但可能和你想要的不太一样，一起来看看吧。 redis的事务可以分为两步，定义事务和执行事务。使用multi命令开启一个事务，然后把要执行的所有命令都依次排上去。这就定义好了一个事务。此时使用exec命令来执行这个事务，或使用discard命令来放弃这个事务。 你可能希望在你的事务开始前，你关心的key不想被别人操作，那么可以使用watch命令来监视这些key，如果开始执行前这些key被其它命令操作了则会取消事务的。也可以使用unwatch命令来取消对这些key的监视。 redis事务具有以下特点： 1、如果开始执行事务前出错，则所有命令都不执行 2、一旦开始，则保证所有命令一次性按顺序执行完而不被打断 3、如果执行过程中遇到错误，会继续执行下去，不会停止的 4、对于执行过程中遇到错误，是不会进行回滚的 看完这些，真想问一句话，你这能叫事务吗？很显然，这并不是我们通常认为的事务，因为它连原子性都保证不了。保证不了原子性是因为redis不支持回滚，不过它也给出了不支持的理由。 不支持回滚的理由： 1、redis认为，失败都是由命令使用不当造成 2、redis这样做，是为了保持内部实现简单快速 3、redis还认为，回滚并不能解决所有问题 哈哈，这就是霸王条款，因此，好像使用redis事务的不太多 如何解决 Redis 的并发竞争 Key 问题 所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） Copyright © 程潇 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 17:29:54 "}}